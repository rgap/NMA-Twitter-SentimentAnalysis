{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Initilization"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["\"\"\"\n","DATA COLLECTION\n","\"\"\"\n","\n","import requests, zipfile, io\n","url = 'https://github.com/rgap/NMA-Twitter-SentimentAnalysis/raw/main/data/raw/trainingandtestdata.zip'\n","raw_data_directory = 'data/raw/'\n","r = requests.get(url)\n","z = zipfile.ZipFile(io.BytesIO(r.content))\n","z.extractall(path=raw_data_directory)\n","\n","\n","# Read the dataset\n","\n","import pandas as pd\n","raw_data_directory = 'data/raw/'\n","# We load the dataset (THIS WILL USE ONLY THE TRAINING DATASET)\n","header_list = [\"polarity\", \"id\", \"date\", \"query\", \"user\", \"text\"]\n","df = pd.read_csv(raw_data_directory + 'training.1600000.processed.noemoticon.csv',\n","                 encoding = \"ISO-8859-1\", names=header_list)\n","# Let's have a look at it\n","df.head(2)\n","\n","\"\"\"\n","DATA PREPROCESSING\n","\"\"\"\n","\n","# Features\n","features = ['id', 'date', 'query', 'user', 'text']\n","# Target\n","target = 'polarity'\n","\n","# Transform the polarity column into just 0s and 1s because it has only 2 unique values and the column type should be int\n","df['polarity'] = df['polarity'].apply(lambda x: 0 if x == 0 else 1)"]},{"cell_type":"markdown","metadata":{},"source":["# 1. Split data into train and test set\n","\n","We analyze only text data, so X will be df['text']"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","X = df.text.values\n","y = df.polarity.values\n","\n","# Split the data into train and test\n","x_train_text, x_test_text, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42,stratify=y)"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Train data size:  1280000\n","Test data size:  320000\n"]}],"source":["# check the size of the train and test data\n","print(\"Train data size: \", len(x_train_text))\n","print(\"Test data size: \", len(x_test_text))"]},{"cell_type":"markdown","metadata":{},"source":["# 2. Feature Engineering"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["# We use CountVectorizer to convert the text into a matrix of token counts\n","# For example if we have the following tweets:\n","# \"I am learning NLP\"\n","# \"NLP is fun\"\n","# The CountVectorizer will convert it into:\n","\n","# [[1, 1, 1, 1, 0],\n","#  [0, 1, 0, 1, 1]]\n","\n","# The columns are the unique words in the text\n","# The values are the count of the word in the text\n","# But it will be in a sparse matrix format. Then it will look like\n","\n","# This will be the first tweet\n","#   (0, 0)\t1\n","#   (0, 1)\t1\n","#   (0, 2)\t1\n","#   (0, 3)\t1\n","\n","# This will be the second tweet\n","#   (1, 1)\t1\n","#   (1, 3)\t1\n","#   (1, 4)\t1\n","\n","# The first column is the row index, the second column is the column index, and the third column is the value\n","# We can convert it to a dense matrix using toarray() method\n","\n","from sklearn.feature_extraction.text import CountVectorizer\n","vectorizer = CountVectorizer()\n","vectorizer.fit(x_train_text)\n","x_train_count_vectorizer = vectorizer.transform(x_train_text)"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["(1280000, 589260)\n"]}],"source":["# print the shape of the matrix\n","print(x_train_count_vectorizer.shape)\n","# the shape is (1280000, 589260) which means we have 1280000 tweets and 589260 unique words"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["@paisleypaisley LOL why do i get ideas so far in advance? it's not even june yet! we need a third knitter to have our own summer group \n"]}],"source":["# print the first row before converting it to a dense matrix\n","print(x_train_text[0])"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["<Compressed Sparse Row sparse matrix of dtype 'int64'\n","\twith 25 stored elements and shape (1, 589260)>\n","  Coords\tValues\n","  (0, 44017)\t1\n","  (0, 165048)\t1\n","  (0, 187921)\t1\n","  (0, 193153)\t1\n","  (0, 213818)\t1\n","  (0, 223808)\t1\n","  (0, 231133)\t1\n","  (0, 247732)\t1\n","  (0, 251948)\t1\n","  (0, 257727)\t1\n","  (0, 283262)\t1\n","  (0, 300176)\t1\n","  (0, 323145)\t1\n","  (0, 379546)\t1\n","  (0, 389097)\t1\n","  (0, 400698)\t1\n","  (0, 401872)\t1\n","  (0, 403800)\t1\n","  (0, 484798)\t1\n","  (0, 501642)\t1\n","  (0, 523180)\t1\n","  (0, 528584)\t1\n","  (0, 559664)\t1\n","  (0, 564428)\t1\n","  (0, 580381)\t1\n","[[0 0 0 ... 0 0 0]]\n"]}],"source":["# print the first row of the matrix  \n","print(x_train_count_vectorizer[0])"]},{"cell_type":"markdown","metadata":{},"source":["# 3. Model Selection\n","\n","Our goal is to train a model capable of estimating the sentiment (POLARITY) of a tweet: 0 or 1\n","\n","So we need a binary classifier."]},{"cell_type":"markdown","metadata":{},"source":["### Testing a Logistic Regression Model"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/homebrew/Caskroom/miniforge/base/envs/twitter_nlp/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy:  0.797578125\n"]}],"source":["# Testing a logistic regression model\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score\n","from sklearn.pipeline import make_pipeline\n","\n","# Create a pipeline\n","model = make_pipeline(CountVectorizer(), LogisticRegression())\n","# Fit the model\n","model.fit(x_train_text, y_train)\n","# Predict the test data\n","y_pred = model.predict(x_test_text)\n","# Calculate the accuracy\n","accuracy = accuracy_score(y_test, y_pred)\n","print(\"Accuracy: \", accuracy)"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0     0.8086    0.7798    0.7939    160000\n","           1     0.7873    0.8154    0.8011    160000\n","\n","    accuracy                         0.7976    320000\n","   macro avg     0.7980    0.7976    0.7975    320000\n","weighted avg     0.7980    0.7976    0.7975    320000\n","\n"]}],"source":["# Checking the classification_report but with precision of 4\n","from sklearn.metrics import classification_report\n","print(classification_report(y_test, y_pred, digits=4))\n","\n","# The accuracy is 0.79 which is not bad for a simple model\n","# But we can improve it by using a more complex model like a\n","# neural network or by tuning the hyperparameters of the model"]},{"cell_type":"markdown","metadata":{},"source":["### Explaining the model results"]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["                   word      coef\n","483052           smiles  2.429968\n","371747      musicmonday  2.487605\n","579089             yayy  2.520072\n","580439              yey  2.590780\n","569457          worries  2.657036\n","91247           blessed  2.727705\n","417326         pleasure  2.789915\n","579106            yayyy  2.799602\n","134081  congratulations  3.041287\n","483142          smiling  3.289673\n","                 word      coef\n","161467  disappointing -4.279824\n","193912        fathers -3.994550\n","104468         bummed -3.831200\n","452580         ruined -3.804651\n","357600         missin -3.749923\n","455520        sadness -3.700784\n","156551     depressing -3.694853\n","225549         gutted -3.651268\n","207419        funeral -3.631763\n","156541      depressed -3.610566\n"]}],"source":["feature_names = model.named_steps['countvectorizer'].get_feature_names_out()\n","# Get the coefficients of the model\n","coefs = model.named_steps['logisticregression'].coef_[0]\n","# Create a dataframe of the coefficients\n","df_coefs = pd.DataFrame({'word': feature_names, 'coef': coefs})\n","# Sort the dataframe by the coefficients\n","df_coefs = df_coefs.sort_values('coef')\n","# Print the top 10 words that have the highest coefficients\n","print(df_coefs.tail(10))\n","# Print the top 10 words that have the lowest coefficients\n","print(df_coefs.head(10))\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# this means that the word \"smiles\" has the highest coefficient and the word \"depressed\" has the lowest coefficient\n","# which means that the word \"smiles\" is more likely to be in a positive tweet and the word \"depressed\" is more likely to be in a negative tweet\n","# We can use these coefficients to understand the model better\n","# The word \"smiles\" has the highest coefficient because it is more likely to be in a positive tweet\n","# The word \"depressed\" has the lowest coefficient because it is more likely to be in a negative tweet"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"twitter_nlp","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"}},"nbformat":4,"nbformat_minor":0}
